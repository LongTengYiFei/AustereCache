2019.05.15
1. LBAIndex, CAIndex <- LBABucket, CABucket <- Bucket.
2. Derived classes of Bucket implement different kinds of cache eviction policy. Specifically, Bucket provide data and interfaces, while derived classes implement them and control on the data.
3. Currently only byte-level allocation is implemented, for finer-grained controll, bit-level memory allocation for Bucket is needed.
4. Google Test framework has been integrated.
2019.05.16
1. Device -> BlockDevice -> DeviceWithCache (Manage two devices)
2. Bring in MurmurHash3 to compute Fingerprint
3. Plan to modify byte-level bucket to bit-level bucket
2019.05.17
1. Modify and test byte-level bucket to bit-level bucket - Done
2. Write the skeleton of write and read of DeviceWithCache
2019.06.03
1. Update architecture graph
2. Implement read/write skeleton and ChunkModule
2019.06.04
MetadataModule::lookup should be splited for write_path and read_path
lookup and update is splited in two stages
Problem met when evicting ca_index entries because the offsets of all entries inside the same bucket change : solved, only contiguous space evicted do we insert the new entry 
2019.06.05
I am implementing the eviction policy of indexing structures
As we have already allocate fixed number of elements in each CABucket and the space for each CABucket is predefined already, I decide to use layout like
-----------------------------------------------------------------
| Elem1: 3 | empty | empty | Elem2: 1 | Elem3: 2 | empty | .... |
-----------------------------------------------------------------
instead of
-----------------------------------------------------------------
| Elem1: 3 | Elem2: 1 | Elem3: 2 | empty | .... | ... |
-----------------------------------------------------------------
As the former one helps when calculate the ssd_location (offset) for each chunk without sacrificing the indexing lookup and update performance.
2019.06.06
Previously, the underlying data are stored in a compacted bitmap.
|----- Bitmap (indexing) ---------------|
Each time when we access the bucket, we firstly copy the corresponding bucket data in the bitmap out.
Bucket is a data structure that is not compacted to support efficient update (really efficient?).
|----- Bucket 0 -------|---- Bucket 1 --|
Then modify-write it.
Now I choose to in-place update the bitmap and organize it not-that-compactedly.
Specifically, each bucket wraps a bitmap, rather than an index wraps a "huge" bitmap.
This might create some wastes because bitmaps are aligned for buckets, but not much at all.
2019.06.11
1. Finish the metadatamodule with verification and journalling needing further amendment when IO module is ready.
2. Tests to the main Index structure has finished, however, the performance needs optimized.
3. I also would like to leave the hit ratio to the future.
4. For the LBA bucket, I currently have no "valid bit" and assume 0 as invalid, which is not right.
   Luckily, this could be easily fixed given that we have existing framework.
5. We might also need real world traces to further test the correctness.
2019.06.12
Read Intel SPDK documents.
SPDK already has examples that create block devices out of NVMe, Memdisk, and Ceph RBD.
One possibility is that we treat SPDK block device the same as the linux one.
The other possibility is that we integrate SPDK into the application to co-design the indexing structure and io channel management.
2019.06.16
1. New verification failed (FP not match) write chunk should invalidate the corresponding FP Bucket.
2. Import openssl library
3. Generate 512MiB trace by randomly generating a chunk-aligned file and cat it. (32MiB (1024 chunks) * 16 files)
4. A concern regarding Hash+memcmp strategy is that when we do compression?
5. Extract all parameters out in common/config.h
2019.06.17
Meeting with Patrick
1. Micro benchmarks for each module to find out performance bottleneck
2. Summarize traces used by other papers (EPLog, Nitro) to find out how to do experiment
Meeting with Wen Xia
1. We tend to implement it as a block device, which needs further investigation and learning
2. For compression module, try LZ4 first.
3. To deal with the fingerprint computation, since nowadays SHA-1 hash computation is fast enough, we may have no need to do memcmp
2019.06.18
1. Compression Module, use LZ4.
